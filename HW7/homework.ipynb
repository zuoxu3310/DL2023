{"cells":[{"cell_type":"markdown","metadata":{"id":"Wlu-s2k9D1Ba"},"source":["### Homework 5: Question search engine\n","\n","Remeber week01 where you used GloVe embeddings to find related questions? That was.. cute, but far from state of the art. It's time to really solve this task using context-aware embeddings.\n","\n","__Warning:__ this task assumes you have seen `seminar.ipynb`!"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-11-09T16:43:04.063931Z","iopub.status.busy":"2023-11-09T16:43:04.063661Z","iopub.status.idle":"2023-11-09T16:43:47.177647Z","shell.execute_reply":"2023-11-09T16:43:47.176791Z","shell.execute_reply.started":"2023-11-09T16:43:04.063906Z"},"id":"HYffoHiI8du5","outputId":"a1effa1f-22cb-4150-bf53-2cf5e7d82762","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\n","Collecting transformers\n","  Downloading transformers-4.35.0-py3-none-any.whl (7.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\n","Collecting datasets\n","  Downloading datasets-2.14.6-py3-none-any.whl (493 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.22.0)\n","Collecting accelerate\n","  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting deepspeed\n","  Downloading deepspeed-0.12.2.tar.gz (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n","Collecting tokenizers<0.15,>=0.14 (from transformers)\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\n","Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.2)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.3.0)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.9.0)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n","Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\n","Collecting hjson (from deepspeed)\n","  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.11.1)\n","Requirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from deepspeed) (9.0.0)\n","Requirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from deepspeed) (1.10.9)\n","Requirement already satisfied: pynvml in /opt/conda/lib/python3.10/site-packages (from deepspeed) (11.4.1)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.6.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Building wheels for collected packages: deepspeed\n","  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.12.2-py3-none-any.whl size=1265684 sha256=dfeeb2d4f7373d4dea0af89862e03f21cd0e15be3f7b60e08a49292e687e3aee\n","  Stored in directory: /root/.cache/pip/wheels/06/c8/39/10f68166de0a2a12c511c3c0569128b62be534edc9a224782c\n","Successfully built deepspeed\n","Installing collected packages: hjson, tokenizers, deepspeed, accelerate, transformers, datasets\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.13.3\n","    Uninstalling tokenizers-0.13.3:\n","      Successfully uninstalled tokenizers-0.13.3\n","  Attempting uninstall: accelerate\n","    Found existing installation: accelerate 0.22.0\n","    Uninstalling accelerate-0.22.0:\n","      Successfully uninstalled accelerate-0.22.0\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.33.0\n","    Uninstalling transformers-4.33.0:\n","      Successfully uninstalled transformers-4.33.0\n","  Attempting uninstall: datasets\n","    Found existing installation: datasets 2.1.0\n","    Uninstalling datasets-2.1.0:\n","      Successfully uninstalled datasets-2.1.0\n","Successfully installed accelerate-0.24.1 datasets-2.14.6 deepspeed-0.12.2 hjson-3.1.0 tokenizers-0.14.1 transformers-4.35.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install --upgrade transformers datasets accelerate deepspeed\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import transformers\n","import datasets"]},{"cell_type":"markdown","metadata":{"id":"HfSHyQlT-fVF"},"source":["### Load data and model"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-11-09T16:43:47.180131Z","iopub.status.busy":"2023-11-09T16:43:47.179573Z","iopub.status.idle":"2023-11-09T16:43:54.436196Z","shell.execute_reply":"2023-11-09T16:43:54.435273Z","shell.execute_reply.started":"2023-11-09T16:43:47.180098Z"},"id":"Y2_wgtrx8e6C","outputId":"5e0dbae4-eaa8-43c6-e59d-34d0745bbdd5","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0e2e2db9e6d246e9b80a57b81512bee6","version_major":2,"version_minor":0},"text/plain":["Downloading readme:   0%|          | 0.00/313 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Repo card metadata block was not found. Setting CardData to empty.\n","/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"db8a17ce32dc4f918adf3ed14a38aab6","version_major":2,"version_minor":0},"text/plain":["Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a1f367171fe042629cee9656a95710c6","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/70.8M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"289e694d9a554f1b9cd571a440bfd466","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/7.83M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8de157d6a4994077827d9ebd20fbdabc","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/76.0M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3c7adc9350bd43ad8474a0851797de3c","version_major":2,"version_minor":0},"text/plain":["Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cc070ce93abc4ae4bcdb16ca75ddf00d","version_major":2,"version_minor":0},"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7cfb8b171fd04f6e9e1702d22acbbed4","version_major":2,"version_minor":0},"text/plain":["Generating validation split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7a011daf4f854817b874f860021d24ef","version_major":2,"version_minor":0},"text/plain":["Generating test split: 0 examples [00:00, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","\n","Sample[0]: {'text1': 'How is the life of a math student? Could you describe your own experiences?', 'text2': 'Which level of prepration is enough for the exam jlpt5?', 'label': 0, 'idx': 0, 'label_text': 'not duplicate'}\n","Sample[3]: {'text1': 'What can one do after MBBS?', 'text2': 'What do i do after my MBBS ?', 'label': 1, 'idx': 3, 'label_text': 'duplicate'}\n"]}],"source":["qqp = datasets.load_dataset('SetFit/qqp')\n","print('\\n')\n","print(\"Sample[0]:\", qqp['train'][0])\n","print(\"Sample[3]:\", qqp['train'][3])"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-11-09T16:43:54.437593Z","iopub.status.busy":"2023-11-09T16:43:54.437270Z","iopub.status.idle":"2023-11-09T16:43:58.838352Z","shell.execute_reply":"2023-11-09T16:43:58.837501Z","shell.execute_reply.started":"2023-11-09T16:43:54.437567Z"},"id":"pStlWcvD8rdk","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9a2225fc186b4eafa3555ce7d8aba8f8","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/320 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"475f6eeab4c440b28a2010a15962711c","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bb875723f2814af684b2226340fa57c2","version_major":2,"version_minor":0},"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3e4d71be68fa45988226325db4925e90","version_major":2,"version_minor":0},"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3c8fbf204224421cb82b003c3a6fcd5c","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/890 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1d6fe9fcc2ce42ec9d231a3fcac0af5b","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/433M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["model_name = \"gchhablani/bert-base-cased-finetuned-qqp\"\n","tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n","model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name)"]},{"cell_type":"markdown","metadata":{"id":"hM3ZujeZ-Z7E"},"source":["### Tokenize the data"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-11-09T16:44:45.710939Z","iopub.status.busy":"2023-11-09T16:44:45.710223Z","iopub.status.idle":"2023-11-09T16:46:45.219124Z","shell.execute_reply":"2023-11-09T16:46:45.218076Z","shell.execute_reply.started":"2023-11-09T16:44:45.710905Z"},"id":"qtkllSPG9bTL","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bd33dd1246c0478baa98ee6310776a5d","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/363846 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5e8cb1e447654c6eaf4eb3bbdb1005e1","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/40430 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6c1f5c3817ce487da6341fa6cd1fd38c","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/390965 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["MAX_LENGTH = 128\n","def preprocess_function(examples):\n","    result = tokenizer(\n","        examples['text1'], examples['text2'],\n","        padding='max_length', max_length=MAX_LENGTH, truncation=True\n","    )\n","    result['label'] = examples['label']\n","    return result\n","\n","qqp_preprocessed = qqp.map(preprocess_function, batched=True)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-11-09T16:46:45.221698Z","iopub.status.busy":"2023-11-09T16:46:45.221294Z","iopub.status.idle":"2023-11-09T16:46:45.230121Z","shell.execute_reply":"2023-11-09T16:46:45.229175Z","shell.execute_reply.started":"2023-11-09T16:46:45.221660Z"},"id":"ObMcFN59_Ll2","outputId":"a6af8bd2-abed-4d2b-bedb-2c0f4ec88583","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[101, 1731, 1110, 1103, 1297, 1104, 170, 12523, 2377, 136, 7426, 1128, 5594, 1240, 1319, 5758, 136,  ...\n"]}],"source":["print(repr(qqp_preprocessed['train'][0]['input_ids'])[:100], \"...\")"]},{"cell_type":"markdown","metadata":{"id":"PyQ1ZbzGAUF2"},"source":["### Task 1: evaluation (1 points)\n","\n","We randomly chose a model trained on QQP - but is it any good?\n","\n","One way to measure this is with validation accuracy - which is what you will implement next.\n","\n","Here's the interface to help you do that:"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-11-09T16:46:45.231413Z","iopub.status.busy":"2023-11-09T16:46:45.231136Z","iopub.status.idle":"2023-11-09T16:46:52.865077Z","shell.execute_reply":"2023-11-09T16:46:52.863988Z","shell.execute_reply.started":"2023-11-09T16:46:45.231389Z"},"id":"M5ueSoieAbBg","trusted":true},"outputs":[],"source":["val_set = qqp_preprocessed['validation']\n","val_loader = torch.utils.data.DataLoader(\n","    val_set, batch_size=1, shuffle=False, collate_fn=transformers.default_data_collator\n",")"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-11-09T16:46:52.868379Z","iopub.status.busy":"2023-11-09T16:46:52.867353Z","iopub.status.idle":"2023-11-09T16:46:53.413368Z","shell.execute_reply":"2023-11-09T16:46:53.412194Z","shell.execute_reply.started":"2023-11-09T16:46:52.868340Z"},"id":"SsPwXXx-At-i","outputId":"fc0f567b-fa26-48ff-d81a-ce7f63f3676d","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Sample batch: {'labels': tensor([0]), 'idx': tensor([0]), 'input_ids': tensor([[  101,  2009,  1132,  2170,   118,  4038,  1177,  2712,   136,   102,\n","          2009,  1132,  1117, 10224,  4724,  1177,  2712,   136,   102,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0]])}\n","\n","Prediction (probs): [[9.99892712e-01 1.07280895e-04]]\n"]}],"source":["for batch in val_loader:\n","     break  # here be your training code\n","print(\"Sample batch:\", batch)\n","\n","with torch.no_grad():\n","  predicted = model(\n","      input_ids=batch['input_ids'],\n","      attention_mask=batch['attention_mask'],\n","      token_type_ids=batch['token_type_ids']\n","  )\n","\n","print('\\nPrediction (probs):', torch.softmax(predicted.logits, dim=1).data.numpy())"]},{"cell_type":"markdown","metadata":{"id":"RoxHzxn0DQqO"},"source":["__Your task__ is to measure the validation accuracy of your model.\n","Doing so naively may take several hours. Please make sure you use the following optimizations:\n","\n","- run the model on GPU with no_grad\n","- using batch size larger than 1\n","- use optimize data loader with num_workers > 1\n","- (optional) use [mixed precision](https://pytorch.org/docs/stable/notes/amp_examples.html)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9k5EK7-KA5F2","outputId":"5eaf5178-0982-48aa-de7b-a21198895c8d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation Accuracy: 0.9084\n"]}],"source":["\n","# <A whole lot of YOUR CODE HERE>\n","# ...\n","\n","\n","# accuracy = <Validation accuracy, between 0 and 1>\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","model.eval()\n","\n","val_set = qqp_preprocessed['validation']\n","val_loader = torch.utils.data.DataLoader(\n","    val_set,\n","    batch_size=64,\n","    shuffle=False,\n","    collate_fn=transformers.default_data_collator,\n","    num_workers=2\n",")\n","\n","correct_predictions = 0\n","total_predictions = 0\n","\n","with torch.no_grad():\n","    for batch in val_loader:\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = model(\n","            input_ids=batch['input_ids'],\n","            attention_mask=batch['attention_mask'],\n","            token_type_ids=batch['token_type_ids']\n","        )\n","\n","        probabilities = torch.softmax(outputs.logits, dim=1)\n","\n","        _, predicted_labels = torch.max(probabilities, dim=1)\n","\n","        correct_predictions += (predicted_labels == batch['labels']).sum().item()\n","        total_predictions += batch['labels'].size(0)\n","accuracy = correct_predictions / total_predictions\n","print(f'Validation Accuracy: {accuracy:.4f}')\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"0R2z_-FZU3qy"},"outputs":[],"source":["assert 0.9 < accuracy < 0.91"]},{"cell_type":"markdown","metadata":{"id":"KONQ1E0J-y6B"},"source":["### Task 2: train the model (5 points)\n","\n","For this task, you have two options:\n","\n","__Option A:__ fine-tune your own model. You are free to choose any model __except for the original BERT.__ We recommend [DeBERTa-v3](https://huggingface.co/microsoft/deberta-v3-base). Better yet, choose the best model based on public benchmarks (e.g. [GLUE](https://gluebenchmark.com/)).\n","\n","You can write the training code manually or use transformers.Trainer (see [this example](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification)). Please make sure that your model's accuracy is at least __comparable__ with the above example for BERT.\n","\n","\n","__Option B:__ compare at least 3 pre-finetuned models (in addition to the above BERT model). For each model, report (1) its accuracy, (2) its speed, measured in samples per second in your hardware setup and (3) its size in megabytes. Please take care to compare models in equal setting, e.g. same CPU / GPU. Compile your results into a table and write a short (~half-page on top of a table) report, summarizing your findings."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-11-09T16:08:34.904769Z","iopub.status.busy":"2023-11-09T16:08:34.904421Z","iopub.status.idle":"2023-11-09T16:08:54.618258Z","shell.execute_reply":"2023-11-09T16:08:54.617248Z","shell.execute_reply.started":"2023-11-09T16:08:34.904737Z"},"id":"u7gcElbWlSr1","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["------Tomor0720/deberta-base-finetuned-qqp-------\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"952744afe3024d439f277b523181cdb6","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3931eaa5cbd04359a5bb8b2cff73d372","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"13df2f6b2c194ddb8b8458393329f3da","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8e0e0afac8fa4eb987b9c2f048e32583","version_major":2,"version_minor":0},"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e6bcfc0b03614d199cd0ab6c6c5ebc6d","version_major":2,"version_minor":0},"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/963 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8116dba2dbed4e458c2f0727b8b2b352","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/787 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"be68c733b2c6416294dacd680d33251f","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/557M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["------JeremiahZ/roberta-base-qqp-------\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"359f0507dcfe488ba2c7df36c261ca09","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/380 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"74268e5783ca49cf9543394ab5e5e7a0","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9f4e5be80c2a4a7ab48d01aad22c3cff","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"59f72601a71e4ea88d2d6db1cde757c4","version_major":2,"version_minor":0},"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b7412da30d104b15930324b3b49e918d","version_major":2,"version_minor":0},"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1a57e0e313624892a225feb58d0924de","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/900 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"29ae6f94b6ab40c098dcc19a8dc5be9b","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["------assemblyai/distilbert-base-uncased-qqp-------\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1845470e9eb14cce8ab0a71e1ba73046","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/306 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f0e2a73860d942619717be57572d9a72","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/537 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bf31106805d7458a8f89495894f5e6b7","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dc6b41943e2447a49210abc6c66764e2","version_major":2,"version_minor":0},"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b4d4d2f3ceab4ded80b7fb7926e13ef9","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["------rajiv003/ernie-finetuned-qqp-------\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"be5660588198417e84cbf8f7a97743a1","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/377 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ed4baeb4a1da4f2fb609deca68c5aa2a","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8af9acb4082e4ef9955d84d87e968758","version_major":2,"version_minor":0},"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5042027d23c545b8b416b1da8b2ed5a2","version_major":2,"version_minor":0},"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dbdde3c3b4f34e7cb679025c6eacdff7","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/790 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4cd6bd774dac432c90ef7f5048d9209e","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["------Alireza1044/albert-base-v2-qqp-------\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1931e72f3bdd4557be0ba6715c94cd2f","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b87bf7b0ddc94c4aaa92997f67f952f5","version_major":2,"version_minor":0},"text/plain":["Downloading spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5edbc0b807ef447d9364286ddd2686ce","version_major":2,"version_minor":0},"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4bca6d79d5964f8c91adff073e78fcda","version_major":2,"version_minor":0},"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/245 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dcd43c437d024e5aa1be123b3b31a372","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/916 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1da7b017f2884c018ac889d6c2ed9315","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/46.8M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoModelForSequenceClassification, AutoTokenizer\n","\n","models = {\n","    \"deberta\":\"Tomor0720/deberta-base-finetuned-qqp\",\n","    \"roberta\": \"JeremiahZ/roberta-base-qqp\",\n","    \"distilbert\": \"assemblyai/distilbert-base-uncased-qqp\",\n","    \"ernie\":\"rajiv003/ernie-finetuned-qqp\",\n","    \"albert\": \"Alireza1044/albert-base-v2-qqp\"\n","}\n","\n","for name, model_name in models.items():\n","    print(f'------{model_name}-------')\n","    newtokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model = AutoModelForSequenceClassification.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-11-09T04:23:32.716611Z","iopub.status.busy":"2023-11-09T04:23:32.715746Z","iopub.status.idle":"2023-11-09T04:23:32.720940Z","shell.execute_reply":"2023-11-09T04:23:32.720006Z","shell.execute_reply.started":"2023-11-09T04:23:32.716577Z"},"trusted":true},"outputs":[],"source":["dataset = qqp['validation']"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":410,"referenced_widgets":["58e1fc17d05348cd90fdcc22069b5a3e","3cb5adb31e3642c59a7eb3484346d658","78344fb74d254e9f88f7b841b55429e7","e026570cf4c746f9b4b85654d192f582","9ba6e5c207fe460da11972ad94db8197","aa95752f7883444088e4fca286d7feb0","99d1b56ef801475abd50a2b9600ff6ea","042b428fee0b4f10b9c24fa66638d1e6","bf715f6800534443879ebd9771673e6f","2ca84e756720402fb3eaed26369f024a","aacdff9b07a7440ebf632a558a05f1db"]},"execution":{"iopub.execute_input":"2023-11-09T04:45:07.736667Z","iopub.status.busy":"2023-11-09T04:45:07.736261Z","iopub.status.idle":"2023-11-09T04:58:37.324264Z","shell.execute_reply":"2023-11-09T04:58:37.322924Z","shell.execute_reply.started":"2023-11-09T04:45:07.736634Z"},"id":"YgJGH_VplXu0","outputId":"ae7e30bd-c4d2-4b75-8e71-49a6525f6979","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2267db6a6d55416d934bdffaf80ea01b","version_major":2,"version_minor":0},"text/plain":["Evaluating:   0%|          | 0/632 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Model deberta Accuracy 0.9127627999010636 Speed (samples/sec) 217.89080187552216\n"]},{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"12f0b945d53e44daaefc931adcf0fcf2","version_major":2,"version_minor":0},"text/plain":["Evaluating:   0%|          | 0/632 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Model roberta Accuracy 0.9153104130596093 Speed (samples/sec) 270.53335354923865\n"]},{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9fb96df8075d4f10aa7ff2a8e86c75ac","version_major":2,"version_minor":0},"text/plain":["Evaluating:   0%|          | 0/632 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Model distilbert Accuracy 0.8992579767499381 Speed (samples/sec) 527.0376819034614\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d0852fb4ec934cb8bb19849078e375b4","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/377 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e180f82da7634385bc7b95ab8e9b44b3","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"284fd8e1b0dd4d149469d194d7eb8f01","version_major":2,"version_minor":0},"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6e0659061e4043858f427ada5f9fed33","version_major":2,"version_minor":0},"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4aeb1682f4bc4a75b7f712fceec9f3be","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/790 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eaf38b671fff4256b5b5da29eaeb8637","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d1a52106448f4c7aa2d72aa42e05bf88","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/40430 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9162297bcb3d4a85b61132fd03508901","version_major":2,"version_minor":0},"text/plain":["Evaluating:   0%|          | 0/632 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Model ernie Accuracy 0.9152114766262677 Speed (samples/sec) 268.5909458562488\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"174c085cbab54e5bb3e153d07611f95b","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"906f399c38314ee6b06e5a918b578951","version_major":2,"version_minor":0},"text/plain":["Downloading spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"906ca1b8c70f4c6899eb692386d4c287","version_major":2,"version_minor":0},"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cc2ebaeeaba741d089a87f94f1e3bfb5","version_major":2,"version_minor":0},"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/245 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0d8acbb353754d5a81d72e730b0ae1b0","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/916 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"01a6092e7cc04bdb9c0fba9b9c391166","version_major":2,"version_minor":0},"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/46.8M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9a42cbaec77c46c3b531e178a83bf9b0","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/40430 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e0a1ea224f6d41fbb86c646bcab3a6d6","version_major":2,"version_minor":0},"text/plain":["Evaluating:   0%|          | 0/632 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Model albert Accuracy 0.9049715557754143 Speed (samples/sec) 236.5968158218924\n","|    | Model      |   Accuracy |   Speed (samples/sec) |\n","|---:|:-----------|-----------:|----------------------:|\n","|  0 | deberta    |   0.912763 |               217.891 |\n","|  1 | roberta    |   0.91531  |               270.533 |\n","|  2 | distilbert |   0.899258 |               527.038 |\n","|  3 | ernie      |   0.915211 |               268.591 |\n","|  4 | albert     |   0.904972 |               236.597 |\n"]}],"source":["import os\n","import time\n","import pandas as pd\n","from tqdm.auto import tqdm\n","from transformers import pipeline\n","\n","def evaluate_model(model, val_loader, device, newtokenizer):\n","    model.eval()\n","    correct_predictions = 0\n","    total_predictions = 0\n","\n","    # Start timing\n","    start_time = time.time()\n","\n","    with torch.no_grad():\n","        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n","              batch = {k: v.to(device) for k, v in batch.items()}\n","              inputs = {\n","                  \"input_ids\": batch[\"input_ids\"],\n","                  \"attention_mask\": batch[\"attention_mask\"]\n","              }\n","              if \"token_type_ids\" in newtokenizer.model_input_names:\n","                  inputs[\"token_type_ids\"] = batch.get(\"token_type_ids\", None)\n","\n","              # Forward pass, get model outputs\n","              outputs = model(**inputs)\n","              probabilities = torch.softmax(outputs.logits, dim=1)\n","\n","              _, predicted_labels = torch.max(probabilities, dim=1)\n","\n","              correct_predictions += (predicted_labels == batch['labels']).sum().item()\n","              total_predictions += batch['labels'].size(0)\n","\n","    # End timing\n","    end_time = time.time()\n","\n","    # Calculate time elapsed and speed\n","    time_elapsed = end_time - start_time\n","    speed = total_predictions / time_elapsed\n","\n","    return correct_predictions / total_predictions, speed\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","results = []\n","\n","for name, model_name in models.items():\n","\n","    newtokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n","\n","    def preprocess_function(examples):\n","        result = newtokenizer(\n","            examples['text1'], examples['text2'],\n","            padding='max_length', max_length=MAX_LENGTH, truncation=True\n","        )\n","        result['label'] = examples['label']\n","        return result\n","    \n","    data_val_preprocessed = dataset.map(preprocess_function, batched=True)\n","\n","    val_loader = torch.utils.data.DataLoader(\n","      data_val_preprocessed,\n","      batch_size=64,\n","      shuffle=False,\n","      collate_fn=transformers.default_data_collator,\n","      num_workers=2\n","    )\n","\n","    accuracy, speed = evaluate_model(model, val_loader, device, newtokenizer)\n","    \n","    print(\"Model\",name,\n","        \"Accuracy\",accuracy,\n","        \"Speed (samples/sec)\",speed\n","         )\n","    \n","    results.append({\n","        \"Model\": name,\n","        \"Accuracy\": accuracy,\n","        \"Speed (samples/sec)\": speed\n","    })\n","\n","df = pd.DataFrame(results)\n","\n","print(df.to_markdown())"]},{"cell_type":"markdown","metadata":{},"source":["## Model Comparison for Duplicate Question Identification\n","\n","### Accuracy\n","From the table, it is evident that **RoBERTa** and **ERNIE** models boast the highest accuracy, both exceeding 91.5%, with RoBERTa marginally leading. \n","- RoBERTa: 91.531%\n","- ERNIE: 91.521%\n","\n","### Speed\n","In terms of inference speed, **DistilBERT** outperforms the others by a significant margin which is nearly double the speed of DeBERTa and ERNIE, and about twice as fast as RoBERTa.\n","\n","### Size\n","About model size, **ALBERT** stands out as the most compact model, occupying a mere 46.8 MB, which is substantially smaller compared to the others, with DistilBERT being the second smallest at 268 MB. DeBERTa is the largest model, requiring 557 MB of storage space.\n","\n","### Summary\n","- For **high accuracy** needs: Choose **RoBERTa** or **ERNIE**.\n","- For a balance between **speed and accuracy**: Opt for **DistilBERT**.\n","- For **resource-constrained environments**: Go with **ALBERT**, accepting a minor compromise on accuracy.\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["|    | Model      |   Accuracy |   Speed (samples/sec) |    Size(MB) |\n","|---:|:-----------|-----------:|----------------------:|------------:|\n","|  0 | deberta    |   0.912763 |               217.891 |         557 |\n","|  1 | roberta    |   0.91531  |               270.533 |         499 |\n","|  2 | distilbert |   0.899258 |               527.038 |         268 |\n","|  3 | ernie      |   0.915211 |               268.591 |         438 |\n","|  4 | albert     |   0.904972 |               236.597 |        46.8 |"]},{"cell_type":"markdown","metadata":{"id":"wQD0IV44LrSs"},"source":["### Task 3: try the full pipeline (2 points)\n","\n","Finally, it is time to use your model to find duplicate questions.\n","Please implement a function that takes a question and finds top-5 potential duplicates in the training set. For now, it is fine if your function is slow, as long as it yields correct results.\n","\n","Showcase how your function works with at least 5 examples."]},{"cell_type":"markdown","metadata":{},"source":["## We use finetuned debertav3 model"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-11-09T16:59:34.930947Z","iopub.status.busy":"2023-11-09T16:59:34.930559Z","iopub.status.idle":"2023-11-09T16:59:36.672348Z","shell.execute_reply":"2023-11-09T16:59:36.671309Z","shell.execute_reply.started":"2023-11-09T16:59:34.930918Z"},"trusted":true},"outputs":[],"source":["model_name = \"Tomor0720/deberta-base-finetuned-qqp\"\n","tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n","model = transformers.AutoModelForSequenceClassification.from_pretrained(model_name)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["## We use **Validation Set** here for testing the naive method"]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2023-11-09T18:30:33.122571Z","iopub.status.busy":"2023-11-09T18:30:33.122180Z","iopub.status.idle":"2023-11-09T18:30:33.127258Z","shell.execute_reply":"2023-11-09T18:30:33.126162Z","shell.execute_reply.started":"2023-11-09T18:30:33.122541Z"},"trusted":true},"outputs":[],"source":["dataset = qqp['validation']"]},{"cell_type":"markdown","metadata":{},"source":["## Naive Method"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2023-11-09T18:30:35.882597Z","iopub.status.busy":"2023-11-09T18:30:35.882198Z","iopub.status.idle":"2023-11-09T18:30:44.792393Z","shell.execute_reply":"2023-11-09T18:30:44.791232Z","shell.execute_reply.started":"2023-11-09T18:30:35.882563Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"52646c6efe844a49b8f2826da78da5e3","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/40430 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["input_question = \"Is Python the best language?\"\n","query = input_question\n","\n","def tokenize_with_query(examples):\n","    return tokenizer(\n","        examples['text1'], [query] * len(examples['text1']),\n","        padding='max_length', max_length=MAX_LENGTH, truncation=True,\n","        return_tensors='pt'\n","    )\n","\n","dataset_with_query = dataset.map(tokenize_with_query, batched=True)"]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2023-11-09T18:44:59.564004Z","iopub.status.busy":"2023-11-09T18:44:59.563271Z","iopub.status.idle":"2023-11-09T18:48:20.125139Z","shell.execute_reply":"2023-11-09T18:48:20.124183Z","shell.execute_reply.started":"2023-11-09T18:44:59.563972Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"61dfa56ceca246fca0db3486a5f937c3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/632 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Score: 0.9977, Text: If more vacuum energy appears with expansion and it has no limit, can infinite of this energy be created? If yes is energy infinite?\n","Score: 0.9971, Text: If universe expands and vacuum energy is created with it (with no limit),is there infinite potential energy/infinite vacuum energy that can be created?\n","Score: 0.9898, Text: *1-(800) 439–2178* BITDEFENDER Antivirus Tech Support Number, BITDEFENDER Antivirus Support Phone Number?\n","Score: 0.9850, Text: If you are traveling in a car going 60mph with a fly inside the car, is the fly traveling 60mph?\n","Score: 0.9850, Text: If you are traveling in a car going 60mph with a fly inside the car, is the fly traveling 60mph?\n"]}],"source":["from tqdm.auto import tqdm\n","\n","def inference_text(model, tokenizer, query, dataset, batch_size=64,topk=5):\n","    model.eval()\n","\n","    train_loader = DataLoader(\n","        dataset, \n","        batch_size=batch_size,\n","        collate_fn=transformers.default_data_collator,\n","        shuffle=False  \n","    )\n","\n","    results = []\n","    with torch.no_grad():\n","        for batch in tqdm(train_loader):\n","            batch = {k: v.to(device) for k, v in batch.items()}\n","            indices = batch.pop('idx').tolist()   \n","            outputs = model(**batch)   \n","\n","            logits = outputs.logits\n","            probs = torch.softmax(logits, dim=1)\n","            scores = probs[:, 1]  \n","            results.extend(zip(indices, scores.tolist()))\n","\n","    results.sort(key=lambda x: x[1], reverse=True)\n","    top_results = results[:topk]\n","    top_duplicates = [(dataset[idx]['text1'],score) for idx, score in top_results]\n","    return top_duplicates\n","\n","top_duplicates = inference_text(model, tokenizer, input_question, dataset_with_query)\n","for text, score in top_duplicates:\n","    print(f\"Score: {score:.4f}, Text: {text}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Optimized Method\n","### Step 1: Create a TF-IDF Vectorizer to Shortlist Candidates\n","### Step 2: Define a Function to Get Top-N Similar Questions Using Cosine Similarity\n","### Step 3: Use the Shortlist in the Transformer Model\n","\n"]},{"cell_type":"code","execution_count":138,"metadata":{"execution":{"iopub.execute_input":"2023-11-09T19:50:25.214728Z","iopub.status.busy":"2023-11-09T19:50:25.213882Z","iopub.status.idle":"2023-11-09T19:50:28.663392Z","shell.execute_reply":"2023-11-09T19:50:28.662621Z","shell.execute_reply.started":"2023-11-09T19:50:25.214692Z"},"trusted":true},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","\n","def preprocess_for_tfidf(texts):\n","    return [text.lower() for text in texts]\n","\n","all_questions = preprocess_for_tfidf([example['text1'] for example in dataset])\n","\n","tfidf_vectorizer = TfidfVectorizer()\n","tfidf_matrix = tfidf_vectorizer.fit_transform(all_questions)"]},{"cell_type":"code","execution_count":139,"metadata":{"execution":{"iopub.execute_input":"2023-11-09T19:50:47.242603Z","iopub.status.busy":"2023-11-09T19:50:47.241610Z","iopub.status.idle":"2023-11-09T19:50:47.247610Z","shell.execute_reply":"2023-11-09T19:50:47.246732Z","shell.execute_reply.started":"2023-11-09T19:50:47.242569Z"},"trusted":true},"outputs":[],"source":["def get_top_n_similar_questions(query, tfidf_matrix, tfidf_vectorizer, top_n=100):\n","    query_vector = tfidf_vectorizer.transform([query.lower()])\n","    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n","    top_n_indices = np.argsort(cosine_similarities)[-top_n:]\n","\n","    return top_n_indices, cosine_similarities[top_n_indices]\n"]},{"cell_type":"code","execution_count":147,"metadata":{"execution":{"iopub.execute_input":"2023-11-09T19:54:03.254843Z","iopub.status.busy":"2023-11-09T19:54:03.253886Z","iopub.status.idle":"2023-11-09T19:54:03.260669Z","shell.execute_reply":"2023-11-09T19:54:03.259633Z","shell.execute_reply.started":"2023-11-09T19:54:03.254808Z"},"trusted":true},"outputs":[],"source":["def inference_text_optimized(model, tokenizer, query, dataset, device, tfidf_matrix, tfidf_vectorizer, topk=5, prefilter_top_n=100):\n","    top_n_indices, _ = get_top_n_similar_questions(query, tfidf_matrix, tfidf_vectorizer, top_n=prefilter_top_n)\n","    shortlisted_dataset = dataset.select(top_n_indices)\n","    dataset_with_query = shortlisted_dataset.map(tokenize_with_query, batched=True)\n","    return inference_text(model, tokenizer, query, dataset_with_query, batch_size=64, topk=topk)\n"]},{"cell_type":"code","execution_count":148,"metadata":{"execution":{"iopub.execute_input":"2023-11-09T19:54:19.446800Z","iopub.status.busy":"2023-11-09T19:54:19.446385Z","iopub.status.idle":"2023-11-09T19:54:20.126496Z","shell.execute_reply":"2023-11-09T19:54:20.125523Z","shell.execute_reply.started":"2023-11-09T19:54:19.446769Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"de160890a01b4bb481287d34b9214b69","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Score: 0.3511, Text: Is Python better than Java? Why or why not?\n","Score: 0.0326, Text: What about c language?\n","Score: 0.0303, Text: What's the difference between a programming language and a scripting language?\n","Score: 0.0125, Text: Why should I learn Python instead of Java?\n","Score: 0.0077, Text: Which is the best source to learn python?\n"]}],"source":["top_duplicates_optimized = inference_text_optimized(model, tokenizer, input_question, dataset, device, tfidf_matrix, tfidf_vectorizer)\n","for text, score in top_duplicates_optimized:\n","    print(f\"Score: {score:.4f}, Text: {text}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["### We can see for the same query it is way more faster than the naive method\n","### Next we do for 5 example questions:\n"]},{"cell_type":"code","execution_count":149,"metadata":{"execution":{"iopub.execute_input":"2023-11-09T20:08:20.311321Z","iopub.status.busy":"2023-11-09T20:08:20.310863Z","iopub.status.idle":"2023-11-09T20:08:20.316254Z","shell.execute_reply":"2023-11-09T20:08:20.315173Z","shell.execute_reply.started":"2023-11-09T20:08:20.311286Z"},"trusted":true},"outputs":[],"source":["dataset = qqp['train']"]},{"cell_type":"code","execution_count":150,"metadata":{"execution":{"iopub.execute_input":"2023-11-09T20:08:22.946903Z","iopub.status.busy":"2023-11-09T20:08:22.946274Z","iopub.status.idle":"2023-11-09T20:08:53.297486Z","shell.execute_reply":"2023-11-09T20:08:53.296417Z","shell.execute_reply.started":"2023-11-09T20:08:22.946873Z"},"trusted":true},"outputs":[],"source":["def preprocess_for_tfidf(texts):\n","    return [text.lower() for text in texts]\n","\n","all_questions = preprocess_for_tfidf([example['text1'] for example in dataset])\n","\n","tfidf_vectorizer = TfidfVectorizer()\n","tfidf_matrix = tfidf_vectorizer.fit_transform(all_questions)"]},{"cell_type":"code","execution_count":151,"metadata":{"execution":{"iopub.execute_input":"2023-11-09T20:09:00.055068Z","iopub.status.busy":"2023-11-09T20:09:00.054702Z","iopub.status.idle":"2023-11-09T20:09:00.059537Z","shell.execute_reply":"2023-11-09T20:09:00.058584Z","shell.execute_reply.started":"2023-11-09T20:09:00.055039Z"},"trusted":true},"outputs":[],"source":["input_questions = [\n","    \"Is Python the best language for data science?\",\n","    \"What is the best way to chill on the weekends?\",\n","    \"Why we need to study?\",\n","    \"What is the best stock\",\n","    \"Where we can find the datasets?\"\n","]"]},{"cell_type":"code","execution_count":152,"metadata":{"execution":{"iopub.execute_input":"2023-11-09T20:09:37.298624Z","iopub.status.busy":"2023-11-09T20:09:37.298222Z","iopub.status.idle":"2023-11-09T20:09:41.329248Z","shell.execute_reply":"2023-11-09T20:09:41.328378Z","shell.execute_reply.started":"2023-11-09T20:09:37.298586Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Input Question: Is Python the best language for data science?\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"78fc4423c3834cfe85eebf71494a9082","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/100 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"207dfea0caa642099f5bac010b0df5f6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Score: 0.9540, Text: Is Python a good first programming language to learn?\n","Score: 0.9125, Text: Is Python really the best programming language for beginners?\n","Score: 0.1195, Text: Which is the best book/resource to learn Python programming language?\n","Score: 0.1089, Text: Should Python be my first programming language?\n","Score: 0.0428, Text: Which is the best programming language to do my project, Python or Java?\n","\n","--------------------------------------------------------------------------------\n","\n","Input Question: What is the best way to chill in the weekends?\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"983a9742abcf4b238f771cb9112cb004","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/100 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae96bc99c1f94f59b359c379b971c17b","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Score: 0.0003, Text: Which is the best way to learn C?\n","Score: 0.0002, Text: What is the the best way to learn programming?\n","Score: 0.0002, Text: What is the the best way to learn programming?\n","Score: 0.0002, Text: What is the the best way to learn programming?\n","Score: 0.0002, Text: What is the the best way to learn programming?\n","\n","--------------------------------------------------------------------------------\n","\n","Input Question: Why we need to study?\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"edb6e10b8ee14341bffd60851fea8de0","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/100 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"80fd72deb9cc4183923bc3ceb288c374","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Score: 0.0021, Text: Why do we need Java programming?\n","Score: 0.0001, Text: How can I clear Gate mech exam, how to study, what to study, and from where to study and how much to study?\n","Score: 0.0001, Text: Why do we need to use Quora when we have Google to search for answers?\n","Score: 0.0001, Text: We can't study everything as it keeps on evolving so what should we study ?\n","Score: 0.0001, Text: Do we need Quora?\n","\n","--------------------------------------------------------------------------------\n","\n","Input Question: What is the best stock\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8015b3284be6416892cba1168abd5b33","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/100 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"77eeaaf1850f45f6b0d37a6bcdef5e76","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Score: 0.0003, Text: What is a stock market? What actually takes place in a stock market?\n","Score: 0.0003, Text: What is a stock market? What actually takes place in a stock market?\n","Score: 0.0001, Text: What is the best stock picking software?\n","Score: 0.0001, Text: Which is the best Indian stock market app?\n","Score: 0.0001, Text: Which is the best Indian stock market app?\n","\n","--------------------------------------------------------------------------------\n","\n","Input Question: Where we can find the datasets?\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0fb8fd0b953c4d7298cff2f7a6024e70","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/100 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e767d55d0d3e416493e166675d0dddd1","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Score: 0.0001, Text: Where Can I find the best seo company?\n","Score: 0.0001, Text: What are the best pop and where can I find it?\n","Score: 0.0001, Text: Which is the best database to store and analyze big datasets in my personal computer?\n","Score: 0.0001, Text: What is love? How can we find that we are in love?\n","Score: 0.0001, Text: What is love? How can we find that we are in love?\n","\n","--------------------------------------------------------------------------------\n","\n"]}],"source":["for input_question in input_questions:\n","    print(f\"Input Question: {input_question}\\n\")\n","    top_duplicates_optimized = inference_text_optimized(\n","        model,\n","        tokenizer,\n","        input_question,\n","        dataset,  \n","        device,\n","        tfidf_matrix,\n","        tfidf_vectorizer\n","    )\n","    for text, score in top_duplicates_optimized:\n","        print(f\"Score: {score:.4f}, Text: {text}\")\n","    print(\"\\n\" + \"-\"*80 + \"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"h86vREj7U02k"},"source":["__Bonus:__ for bonus points, try to find a way to run the function faster than just passing over all questions in a loop. For isntance, you can form a short-list of potential candidates using a cheaper method, and then run your tranformer on that short list. If you opted for this solution, please keep both the original implementation and the optimized one - and explain briefly what is the difference there."]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"widgets":{"application/vnd.jupyter.widget-state+json":{"042b428fee0b4f10b9c24fa66638d1e6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ca84e756720402fb3eaed26369f024a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3cb5adb31e3642c59a7eb3484346d658":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa95752f7883444088e4fca286d7feb0","placeholder":"​","style":"IPY_MODEL_99d1b56ef801475abd50a2b9600ff6ea","value":"Evaluating:   9%"}},"58e1fc17d05348cd90fdcc22069b5a3e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3cb5adb31e3642c59a7eb3484346d658","IPY_MODEL_78344fb74d254e9f88f7b841b55429e7","IPY_MODEL_e026570cf4c746f9b4b85654d192f582"],"layout":"IPY_MODEL_9ba6e5c207fe460da11972ad94db8197"}},"78344fb74d254e9f88f7b841b55429e7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_042b428fee0b4f10b9c24fa66638d1e6","max":632,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bf715f6800534443879ebd9771673e6f","value":60}},"99d1b56ef801475abd50a2b9600ff6ea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9ba6e5c207fe460da11972ad94db8197":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa95752f7883444088e4fca286d7feb0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aacdff9b07a7440ebf632a558a05f1db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf715f6800534443879ebd9771673e6f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e026570cf4c746f9b4b85654d192f582":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ca84e756720402fb3eaed26369f024a","placeholder":"​","style":"IPY_MODEL_aacdff9b07a7440ebf632a558a05f1db","value":" 60/632 [01:05&lt;09:52,  1.04s/it]"}}}}},"nbformat":4,"nbformat_minor":4}
